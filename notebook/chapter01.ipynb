{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579556f9",
   "metadata": {},
   "source": [
    "## Introduction to Bandit Algorithms\n",
    "This chapter provides an introduction to multi-armed bandits and their practical applications.\n",
    "\n",
    "### The Multi-Armed Bandit Problem - An Intuitive Scenario\n",
    "Imagine you walked into a casino filled with a rows of slot machines. \n",
    "\n",
    "Each machine (or arms) has its own secret payout rate -- some pay out often but in small amount, others rarely but with big prizez, and some might be plain terrible. The problem is: **you do not know these pay out rates in advance**. \n",
    "\n",
    "Every time you pull the lever on a machine, you get a payoff -- maybe a win, maybe nothing -- and you learn a bit about a machine's behavior. Your goal is simple: **win as much as possible over many plays**.\n",
    "\n",
    "This creates a deliemma:\n",
    "\n",
    "- **Exploration**: You might try different machines to discover which ones are worth playing. \n",
    "\n",
    "- **Exploitation**: One you find a machine that seems to play well, you might keep playing it to colect more rewards.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7078c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b>\n",
    "    <ul>\n",
    "        <li>If you only <b>explore</b> you will waste your time on bad machines.</li>\n",
    "        <li>If you only <b>exploit</b>, you might miss discovering an even better one.</li>\n",
    "    </ul>\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340df5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### The language of Bandits\n",
    "\n",
    "A bandit problem is like a repetitive game between two sides:\n",
    "- **The learner** (or agent): That is you or your algorithm. Example: a player choosing which slot machine to pull.\n",
    "- **The environment** : That is the world you are interacting with. Example: the slot machines themselves, each with its own unknown payout distribution.\n",
    "#### Problem statement\n",
    "The game happens over a fixed number of turns, called the **horizon**. We use **$n$** for the number of turns ($n$ is a positive natural number. e.g, 1,2,3,...).\n",
    "\n",
    "In each round $t \\in [n]$ :\n",
    "\n",
    "1. In each round, the learner choose an action $A_t$ from a given set $A$. *Example: picking which slot machine to play*.\n",
    "\n",
    "2. The environment gives you a feedback in the form of a reward $X_t$ -- maybe you win coins, maybe nothing.   \n",
    "\n",
    "At each time step $t$ we have a **history** of what has happened so far: $$H_t =(A_1, X_1, A_2, X_2, .., A_{t-1}, X_{t-1} )$$.\n",
    "A policy is a rule (or function) that maps history $H_t$ to the next action $A_t$. We typically denote a policy by the symblo $\\pi$.\n",
    "\n",
    "The goal is to find a policy ($\\pi$) that maximize totalcummulative reward over $n$ rounds: $$\\sum_{t=1}^{n} X_t$$. \n",
    "\n",
    "##### Evaluating the learner\n",
    "We need a way to evaluate the learner, there are plenty of performance measures. Here we introduce **regret**.\n",
    "\n",
    "##### Regret\n",
    "In bandit problems, regret is a way to measure how much reward you miss out on *by not following* the best possible policy.\n",
    "\n",
    "Formally, suppose we compare our learner to a competitor class of policies denotr $\\prod$. Each $\\pi \\in \\prod$ is a possible policy we might have followed.\n",
    "\n",
    "The **regret** of the learner with respect to a single policy $\\pi$ is:\n",
    "\n",
    "$$R_n(\\pi) = E[\\sum_{t=1}^{n} X_{t}^{\\pi}] - E[\\sum_{t=1}^{n} X_{t}^{learner}]$$\n",
    "\n",
    "- $X_{t}^{\\pi}$ = the reward at time $t$, if we follow the policy $\\pi$.\n",
    "- $X_{t}^{learner}$ = the reward at time $t$ with the learner's actual actions.\n",
    "\n",
    "If we measure regret to a set of policies $\\prod$, we take maximum regret over all policies ib $\\prod$:\n",
    "\n",
    "$$R_n(\\pi) = \\max_{\\pi \\in \\prod} R_n(\\pi)$$\n",
    "\n",
    "In many cases, $\\prod$ is chosen large enogh, so it contains the optimal policy for all environments in a set $\\varepsilon$.\n",
    "\n",
    "- Here $\\varepsilon$ represent the possible worlds (environments) we might be in.\n",
    "- In this setting, the regret tells us how much we lose compared to the optimal policy, even without knowing which environment we’re in ahead of time.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Intuition:</b>\n",
    "<hr/>\n",
    "If your regret is <b>zero</b>, it means you played just as well as the best possible strategy could have played (given the environment). If your regret is <b>large</b>, it means you missed out on a lot of possible reward.\n",
    "</div>\n",
    "\n",
    "### Applications of Bandit Algorithms\n",
    "\n",
    "#### A/B Testing\n",
    "\n",
    "Traditionally, A/B testing splits visitors evenly between two version of the websites (e.g., \"buy now\" button \"$SiteA$\" at the top vs at the bottom \"$SiteB$\" ) and waits until the end to see which works better. This wastes time if one version is clearly better early on.\n",
    "\n",
    "Using a bandit approach, each visitor is treated as a round of the game. At each time $t$, the bandit algorithms decides which version of the website ($SiteA$ ot $SiteB$) shows to the user based on the previous choices $H_t$. So $A_t$ should be chosen between $A=\\{SiteA, SiteB\\}$ and then reward is $1$ ($X_t = 1$) if the user does purchase and $0$ ($X_t =0$) otherwise.\n",
    "\n",
    "#### Advert Placement \n",
    "Consider a user visiting a website that include adverts. For this problem we can state bandit algorithm as below:\n",
    "\n",
    "- **Setting**: Each round $t$ = one user visits a website.\n",
    "- **Actions**: \n",
    "    $$ A = \\{ \\text{all available adverts} \\} $$\n",
    "- **Action chosen at round $t$** $$A_t \\in A$$\n",
    "- **Reward:**\n",
    "    $$ X_t = \\begin{cases} 1 & \\text{if user clicks the advert} \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Note:</b>\n",
    "    In websites like amazon, advertising are often targeted For example, a user who recently purchased rock-climbing shoes is much more likely to buy a harness than another random user. Clearly, an algorithm should take this context into account. In the next chapter we will tell you how to use this side information to make the learner better.\n",
    "</div>\n",
    "\n",
    "#### Recommendation Service\n",
    "Consider Netflix movie recommendation system as a major example. Netflix faces the challenge of deciding which movies to show most prominently on your browser page. Key points:\n",
    "\n",
    "1. **Sequential decisions (actions)**: Users arrive one by one, and Netflix must choose what to show each time.\n",
    "\n",
    "2. **Rewards (possible rewards)**: Measured by things like:\n",
    "  - (a) Whether you watch the movie ($X_t =1$ if watch the movie and $0$ otherwise)\n",
    "  - (b) Whether you rated it positively.\n",
    "3. **Challenges**:\n",
    "   - **Large action space**- Many possible movie combinations to display.\n",
    "   - **Sparse data per user**- Each user watches only a few movies, and preferences vary widely. \n",
    "   - **Data collection is interactive**- What Netflix recommend influences what data they collect. Example: If Netflix never recommend AlphaGo movie, few people will watch it, and data abourt it will renmain scarce.\n",
    "\n",
    "#### Network Routing\n",
    "The network routing problem involves determining the most efficient path for data packets to travel across a network from source to destination.\n",
    "\n",
    "- **Goal**: Direct internet traffic through the shortest path in a network. \n",
    "- **Action set ($A$)**: All possible paths between source and destination.\n",
    "- **Action at each round ($A_t$)**: A path based on previous choices ($H_t$)\n",
    "- **Reward ($X_t$)**: Negative of travel time (*shorter = higher reward*)\n",
    "\n",
    "It includes the below challenges:\n",
    "- Combinatorially large action space\n",
    "- Many possible paths → hard to explore all.\n",
    "\n",
    "#### Dynamic Rewarding\n",
    "\n",
    "Dynamic pricing is a strategy where the price of a product changes over time to maximize revenue or profit, often adapting to customer demands, market trends or inventory level. \n",
    "\n",
    "- **Action set ($A$)**: Set of possible prices \n",
    "- **Action at each round ($A_t$)**: Price set at round $t$ (learner should select price and assign to the product(s)).\n",
    "- **Reward ($X_t$)**: Reward at round $t$ (e.g., profit or revenue)\n",
    "\n",
    "#### Waiting problems\n",
    "Waiting problems involve deciding how long to wait for an uncertain event before taking an alternative action, aimimg to minimize total cost or time. \n",
    "\n",
    "*Example*: Deciding how long to wait for a bus (the bus may arrive too late or even not arrive) before walking to work. \n",
    "\n",
    "- **Action set ($A$)**: Set of possible waiting times before switching to the alternative action (e.g., start walking)\n",
    "- **Action at each round ($A_t$)**: Waiting time at round $t$.\n",
    "- **Reward ($X_t$)**: negative of total travel time\n",
    "\n",
    "**Challengs**:\n",
    "- if you walk to soon -> miss the bus, gain little information.\n",
    "- if you wait too long -> waste time, arrive late.\n",
    "- Data is **censored**: On days you leave before the bus arrive, you do not observe its actual arrive time.\n",
    "\n",
    "**Applications**\n",
    "- Powering off the car engine at traffic lights.\n",
    "- Putting hard drive to sleep after inactivity.\n",
    "\n",
    "#### Resource Allocation\n",
    "Resource allocation problems involve distributing limited resources among competing tasks or locations to maximize performance or profit.\n",
    "\n",
    "When demand or supply is uncertain, the problem resembles a bandit setting.\n",
    "\n",
    "- **Action set ($A$)**: Set of possible allocation strategies (e.g., units of resource to assign to each task).\n",
    "- **Action at each round ($A_t$)**: Resource distribution chosen at round $t$.\n",
    "- **Reward ($X_t$)**: Output, revenue, or efficiency achieved from the allocation.\n",
    "\n",
    "**Challenge**\n",
    "- Allocating too few resources → under-serves demand and reveals little about true needs.\n",
    "- Allocating too many resources → wasteful use and missed opportunities elsewhere.\n",
    "\n",
    "**Applications**\n",
    "- Allocating servers to web applications with uncertain traffic.\n",
    "- Distributing medical staff across hospital departments.\n",
    "- Assigning ad budgets to marketing channels.\n",
    "\n",
    "#### Tree Search (UCT Algorithm)\n",
    "The **Upper Confidence bounds applied to Trees (UCT)** algorithm is a tree search method widely used in perfect-information games (e.g., Go, Chess).\n",
    "It builds a search tree iteratively by:\n",
    "\n",
    "1. Selection – choosing a path from the root to a leaf.\n",
    "\n",
    "2. Expansion – adding new nodes (if possible).\n",
    "\n",
    "3. Simulation – running a Monte Carlo roll-out to the game’s end.\n",
    "\n",
    "\n",
    "- Action set ($A$): Possible moves (children) from the current node.\n",
    "\n",
    "- Action at each round ($A_t$): Move selected at node $t$ during tree traversal.\n",
    "\n",
    "- Reward ($X_t$): Game outcome or simulated reward from the roll-out.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
